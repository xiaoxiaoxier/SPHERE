{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018887fd-e9e8-495b-872f-fefbd9cd6cb5",
   "metadata": {},
   "source": [
    "To generate synthetic VisiumHD data from Xenium, please read and run all the cells below. Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c610b-e1b5-43e6-a35d-3548588cb652",
   "metadata": {},
   "source": [
    "## Download Xenium output from 10X website\n",
    "Paste the URL for the binned_outputs.tar.gz for the sample you want to analyze.\n",
    "\n",
    "1. Go to Xenium public datasets page:https://www.10xgenomics.com/datasets?query=&page=1&configure%5BhitsPerPage%5D=50&configure%5BmaxValuesPerFacet%5D=1000&refinementList%5Bproduct.name%5D%5B0%5D=In%20Situ%20Gene%20Expression&refinementList%5Bspecies%5D%5B0%5D=Human&refinementList%5BdiseaseStates%5D%5B0%5D=colorectal%20cancer\n",
    "\n",
    "2. Select sample to analyze scrolling down to downloads section, click \"Batch download\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fcd48a-2f55-43b4-befd-8d646ea634cf",
   "metadata": {},
   "source": [
    "### Install prerequisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a54ee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed. Files are extracted to: /home/wangzhuo/data/Xenium_Human_Colorectal_Cancer\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# 指定 ZIP 文件的路径\n",
    "zip_file_path = \"/home/wangzhuo/data/Xenium_Human_Colorectal_Cancer/Xenium_V1_Human_Colorectal_Cancer_Addon_FFPE_outs.zip\"\n",
    "\n",
    "# 解压 ZIP 文件到指定目录\n",
    "extract_dir = \"/home/wangzhuo/data/Xenium_Human_Colorectal_Cancer\"  # 可以修改为你想要的解压目录\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Extraction completed. Files are extracted to: {extract_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79fb2c-0fd9-4bd4-8be9-4d1bd04d8733",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e4dc02-2b8d-4e00-9cbd-8a4d151ca5af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd # Geopandas for storing Shapely objects\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import anndata\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import shapely\n",
    "from shapely.geometry import Polygon, Point # Representing bins and cells as Shapely Polygons and Point objects\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8d90a-65dd-4e93-b4e2-4a257d6e1dc7",
   "metadata": {},
   "source": [
    "### Load Cell & Transcripts Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb54b91-6757-467c-81d3-7a4f6916fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transcript data\n",
    "transcripts_path = \"/home/wangzhuo/data/Xenium_Human_Colorectal_Cancer/transcripts.csv.gz\"\n",
    "with gzip.open(transcripts_path, 'rt') as f:\n",
    "    transcripts_df = pd.read_csv(f)\n",
    "\n",
    "# Load cell info\n",
    "cells_path = \"/home/wangzhuo/data/Xenium_Human_Colorectal_Cancer/cells.csv.gz\"\n",
    "with gzip.open(cells_path, 'rt') as f:\n",
    "    cells_df = pd.read_csv(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac1dea-7855-4af4-8989-c2b63deed2f1",
   "metadata": {},
   "source": [
    "### Load Cell Boundary Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d2bdf0-8871-4bb0-a38e-3f9c31c7b3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      " ├── cell_id (388175, 2) uint32\n",
      " ├── cell_summary (388175, 8) float64\n",
      " ├── masks\n",
      " │   ├── 0 (20493, 51115) uint32\n",
      " │   ├── 1 (20493, 51115) uint32\n",
      " │   └── homogeneous_transform (4, 4) float32\n",
      " └── polygon_sets\n",
      "     ├── 0\n",
      "     │   ├── cell_index (388175,) uint32\n",
      "     │   ├── method (388175,) uint32\n",
      "     │   ├── num_vertices (388175,) int32\n",
      "     │   └── vertices (388175, 50) float32\n",
      "     └── 1\n",
      "         ├── cell_index (388175,) uint32\n",
      "         ├── method (388175,) uint32\n",
      "         ├── num_vertices (388175,) int32\n",
      "         └── vertices (388175, 50) float32\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "zarr_file = zarr.open('/home/wangzhuo/data/Xenium_Human_Colorectal_Cancer/cells.zarr.zip', mode='r')\n",
    "print(zarr_file.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb168c2",
   "metadata": {},
   "source": [
    "细胞核顶点数据\t人 zarr_file['polygon_sets/0/vertices'][:]\t小鼠 zarr_file['polygon_vertices'][0, :, :]\n",
    "\n",
    "整个细胞顶点数据\t人 zarr_file['polygon_sets/1/vertices'][:] 小鼠\tzarr_file['polygon_vertices'][1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c092c013-dd0d-47f5-a6cc-3491f1f62dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = zarr_file['polygon_sets/0/vertices'][:]\n",
    "# 1 is whole cell, 0 is nucleus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5ff74-6269-42b4-9a9e-604f520a7528",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create folders to store synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6839176-4a75-4f1e-b4f7-13899b946963",
   "metadata": {},
   "source": [
    "For both the `seqfish_dir` and `enact_data_dir`, change `\"/home/oneai/\"` to the directory that stores this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec69f53-4a93-491a-b6f0-652b27ffaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenium_dir = \"/home/wangzhuo/data/enact_synthetic_output/chunks\" # Update it to the directory where you want to save the synthetic data\n",
    "enact_data_dir = \"/home/wangzhuo/data/enact_synthetic_output/chunks\" # Directory that saves all the input and results of the enact pipeline, \n",
    "# should end with \"oneai-dda-spatialtr-visiumhd_analysis/cache/seqfish/chunks\"\n",
    "\n",
    "transcripts_df_chunks_dir = os.path.join(xenium_dir, \"transcripts_patches\") # Directory to store the files that contain the transcripts info for each chunk\n",
    "output_dir = os.path.join(enact_data_dir, \"bins_gdf\") # Directory to store the results of gene-to-bin assignment for each chunk\n",
    "cells_df_chunks_dir =  os.path.join(enact_data_dir,\"cells_gdf\") \n",
    "ground_truth_dir =  os.path.join(xenium_dir, \"ground_truth_nuclei\")\n",
    "\n",
    "# Making relevant directories\n",
    "os.makedirs(xenium_dir, exist_ok=True)\n",
    "os.makedirs(enact_data_dir, exist_ok=True)\n",
    "os.makedirs(transcripts_df_chunks_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(cells_df_chunks_dir, exist_ok=True)\n",
    "os.makedirs(ground_truth_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe70a1-ed23-4cb6-a7b6-d35e4c01f895",
   "metadata": {},
   "source": [
    "### Generate Synthetic VisiumHD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd8461-7bcc-4101-b26b-765daf975916",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Break transcripts df to patches (based on location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b4ce0-30d1-4c23-9b2d-0622db0a4f8c",
   "metadata": {},
   "source": [
    "Break transcripts df to patches of size 1000um x 1000um (larger patch size may result in memory issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60fb886a-5893-40ba-b187-650d6cfb4ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved patch_0_0.csv\n",
      "Saved patch_0_1.csv\n",
      "Saved patch_0_2.csv\n",
      "Saved patch_0_3.csv\n",
      "Saved patch_1_0.csv\n",
      "Saved patch_1_1.csv\n",
      "Saved patch_1_2.csv\n",
      "Saved patch_1_3.csv\n",
      "Saved patch_2_0.csv\n",
      "Saved patch_2_1.csv\n",
      "Saved patch_2_2.csv\n",
      "Saved patch_2_3.csv\n",
      "Saved patch_3_0.csv\n",
      "Saved patch_3_1.csv\n",
      "Saved patch_3_2.csv\n",
      "Saved patch_3_3.csv\n",
      "Saved patch_4_0.csv\n",
      "Saved patch_4_1.csv\n",
      "Saved patch_4_2.csv\n",
      "Saved patch_4_3.csv\n",
      "Saved patch_4_4.csv\n",
      "Saved patch_5_0.csv\n",
      "Saved patch_5_1.csv\n",
      "Saved patch_5_2.csv\n",
      "Saved patch_5_3.csv\n",
      "Saved patch_5_4.csv\n",
      "Saved patch_6_0.csv\n",
      "Saved patch_6_1.csv\n",
      "Saved patch_6_2.csv\n",
      "Saved patch_6_3.csv\n",
      "Saved patch_6_4.csv\n",
      "Saved patch_7_0.csv\n",
      "Saved patch_7_1.csv\n",
      "Saved patch_7_2.csv\n",
      "Saved patch_7_3.csv\n",
      "Saved patch_7_4.csv\n",
      "Saved patch_8_0.csv\n",
      "Saved patch_8_1.csv\n",
      "Saved patch_8_2.csv\n",
      "Saved patch_8_3.csv\n",
      "Saved patch_8_4.csv\n",
      "Saved patch_9_0.csv\n",
      "Saved patch_9_1.csv\n",
      "Saved patch_9_2.csv\n",
      "Saved patch_9_3.csv\n",
      "Saved patch_9_4.csv\n",
      "Saved patch_10_1.csv\n",
      "Saved patch_10_2.csv\n",
      "Saved patch_10_3.csv\n",
      "Saved patch_10_4.csv\n"
     ]
    }
   ],
   "source": [
    "# patch size: 1000 um x 1000 um\n",
    "\n",
    "patch_size = 1000\n",
    "\n",
    "# patch indices\n",
    "transcripts_df['x_patch'] = (transcripts_df['x_location'] // patch_size).astype(int)\n",
    "transcripts_df['y_patch'] = (transcripts_df['y_location'] // patch_size).astype(int)\n",
    "transcripts_df[\"patch_id\"] = transcripts_df[\"x_patch\"].astype(str) + \"_\" + transcripts_df[\"y_patch\"].astype(str)\n",
    "\n",
    "# Create a df for each patch\n",
    "grouped = transcripts_df.groupby(['x_patch', 'y_patch'])\n",
    "for (x_patch, y_patch), group in grouped:\n",
    "    # Calculate the start and end locations for each patch\n",
    "    # x_start = x_patch * patch_size\n",
    "    # x_end = (x_patch + 1) * patch_size\n",
    "    # y_start = y_patch * patch_size\n",
    "    # y_end = (y_patch + 1) * patch_size\n",
    "    \n",
    "    filename = f\"patch_{x_patch}_{y_patch}.csv\"\n",
    "    output_loc = os.path.join(transcripts_df_chunks_dir , filename)\n",
    "    group.to_csv(output_loc)\n",
    "\n",
    "    print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbc9ec-675b-4b25-8448-334ed317798a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate synthetic visiumHD for each patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebc8fd-88e9-4b14-8470-a474085dee64",
   "metadata": {},
   "source": [
    "Each patch is broken into bins of size 2um x 2um. The synthetic data contains transcript counts orgnized by bin_id. Each row contains transcript counts for a unique bin. Bins with no transcript counts is not included. \n",
    "\n",
    "In addition to all the gene features, there are two additional columns represent the row number and column number of the bin, and a column contains the Shapely polygon item that represents the bin. The first column is the bin_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19155a0-5646-49bd-915c-94737e251bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_VisiumHD_data(transcripts_df, bin_size=2, whole_cell=True, QScore20=True):\n",
    "    filtered_df = transcripts_df.copy()\n",
    "    # only count transcripts in the nucleus\n",
    "    if not whole_cell:\n",
    "        filtered_df = transcripts_df[transcripts_df['overlaps_nucleus'] == 1].copy()\n",
    "    \n",
    "    #only count transcripts with QScore >= 20\n",
    "    if QScore20:\n",
    "        filtered_df = filtered_df[filtered_df['qv'] >= 20].copy()\n",
    " \n",
    "    # assigne bin to each transcript\n",
    "    filtered_df.loc[:, 'row'] =np.ceil(filtered_df['y_location'] / bin_size).astype(int)\n",
    "    filtered_df.loc[:, 'column'] = np.ceil(filtered_df['x_location'] / bin_size).astype(int)\n",
    "    filtered_df.loc[:, 'assigned_bin_id'] = filtered_df.apply(\n",
    "        lambda row: f\"{bin_size}um_\" + str(row['row']).zfill(5) +\"_\"+ str(row['column']).zfill(5),\n",
    "        axis=1)\n",
    "    \n",
    "    bin_coordinates = filtered_df[['assigned_bin_id', 'row', 'column']].drop_duplicates().set_index('assigned_bin_id')\n",
    "    bin_gene_matrix = filtered_df.groupby(['assigned_bin_id', 'feature_name']).size().unstack(fill_value=0)\n",
    "    bin_gene_matrix_with_coords = bin_gene_matrix.merge(bin_coordinates, left_index=True, right_index=True)\n",
    "    \n",
    "    return bin_gene_matrix_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd804c49-dc85-4fa9-85d4-a621cf0598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract row and column number from the bin_id\n",
    "def extract_numbers(entry):\n",
    "    match = re.search(r'_(\\d{5})_(\\d{5})', entry)\n",
    "    if match:\n",
    "        number1 = int(match.group(1).lstrip('0'))  \n",
    "        number2 = int(match.group(2).lstrip('0'))  \n",
    "        return number2*2-1, number1*2-1\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8d45c22-2776-4b80-a29b-37d07f6b06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_bin_polys(bins_df, x_col, y_col, bin_size):\n",
    "        \"\"\"Represents the bins as Shapely polygons\n",
    "\n",
    "        Args:\n",
    "            bins_df (pd.DataFrame): bins dataframe\n",
    "            x_col (str): column with the bin centre x-coordinate\n",
    "            y_col (str): column with the bin centre y-coordinate\n",
    "            bin_size (int): bin size in pixels\n",
    "\n",
    "        Returns:\n",
    "            list: list of Shapely polygons\n",
    "        \"\"\"\n",
    "        geometry = []\n",
    "        # Generates Shapely polygons to represent each bin\n",
    "\n",
    "        if True:\n",
    "            half_bin_size = bin_size / 2\n",
    "            bbox_coords = pd.DataFrame(\n",
    "                {\n",
    "                    \"min_x\": bins_df[x_col] - half_bin_size,\n",
    "                    \"min_y\": bins_df[y_col] - half_bin_size,\n",
    "                    \"max_x\": bins_df[x_col] + half_bin_size,\n",
    "                    \"max_y\": bins_df[y_col] + half_bin_size,\n",
    "                }\n",
    "            )\n",
    "            geometry = [\n",
    "                shapely.geometry.box(min_x, min_y, max_x, max_y)\n",
    "                for min_x, min_y, max_x, max_y in tqdm(\n",
    "                    zip(\n",
    "                        bbox_coords[\"min_x\"],\n",
    "                        bbox_coords[\"min_y\"],\n",
    "                        bbox_coords[\"max_x\"],\n",
    "                        bbox_coords[\"max_y\"],\n",
    "                    ),\n",
    "                    total=len(bins_df),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693d7f0",
   "metadata": {},
   "source": [
    "下面这段代码的主要任务是：\n",
    "\n",
    "处理多个转录本数据文件（chunks）。\n",
    "\n",
    "对每个转录本数据文件进行筛选和分配到bin中。\n",
    "\n",
    "\n",
    "\n",
    "调整bin的行列坐标。\n",
    "将bin转换为Shapely多边形对象。\n",
    "\n",
    "创建包含地理空间信息的GeoDataFrame。\n",
    "\n",
    "将处理后的数据保存为CSV文件，并输出成功信息。\n",
    "这段代码适用于需要对大量单细胞空间转录组数据进行批量处理和空间分析的场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6936c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29564/29564 [00:00<00:00, 115719.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_0_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116474/116474 [00:00<00:00, 134019.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_0_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114212/114212 [00:00<00:00, 129463.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_0_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48869/48869 [00:00<00:00, 138620.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_0_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86057/86057 [00:00<00:00, 132388.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_1_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207381/207381 [00:01<00:00, 134950.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_1_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163887/163887 [00:01<00:00, 133049.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_1_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74159/74159 [00:00<00:00, 121607.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_1_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164052/164052 [00:01<00:00, 126297.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_2_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185760/185760 [00:01<00:00, 132233.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_2_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174597/174597 [00:01<00:00, 131895.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_2_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99432/99432 [00:00<00:00, 116036.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_2_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150496/150496 [00:01<00:00, 123305.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_3_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190170/190170 [00:01<00:00, 133733.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_3_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212488/212488 [00:01<00:00, 127868.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_3_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129728/129728 [00:01<00:00, 123264.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_3_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168501/168501 [00:01<00:00, 133565.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_4_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208910/208910 [00:01<00:00, 123725.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_4_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173329/173329 [00:01<00:00, 136394.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_4_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80077/80077 [00:00<00:00, 131149.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_4_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4984/4984 [00:00<00:00, 138467.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_4_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177991/177991 [00:01<00:00, 124432.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_5_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171183/171183 [00:01<00:00, 135003.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_5_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182367/182367 [00:01<00:00, 129000.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_5_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122488/122488 [00:00<00:00, 141821.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_5_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51598/51598 [00:00<00:00, 122326.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_5_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164754/164754 [00:01<00:00, 134757.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_6_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171924/171924 [00:01<00:00, 127001.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_6_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174107/174107 [00:01<00:00, 125051.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_6_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194064/194064 [00:01<00:00, 124978.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_6_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61402/61402 [00:00<00:00, 139783.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_6_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185415/185415 [00:01<00:00, 128060.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_7_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228841/228841 [00:01<00:00, 137500.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_7_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186837/186837 [00:01<00:00, 127801.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_7_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182744/182744 [00:01<00:00, 128361.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_7_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64029/64029 [00:00<00:00, 140462.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_7_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49661/49661 [00:00<00:00, 139690.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_8_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193570/193570 [00:01<00:00, 133601.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_8_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200388/200388 [00:01<00:00, 123875.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_8_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175617/175617 [00:01<00:00, 132072.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_8_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16818/16818 [00:00<00:00, 139676.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_8_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2312/2312 [00:00<00:00, 138783.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_9_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 67036/67036 [00:00<00:00, 141940.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_9_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191105/191105 [00:01<00:00, 131499.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_9_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225490/225490 [00:01<00:00, 122780.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_9_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12710/12710 [00:00<00:00, 141762.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_9_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8766/8766 [00:00<00:00, 134508.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_10_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80502/80502 [00:00<00:00, 141469.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_10_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121132/121132 [00:00<00:00, 129271.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_10_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2829/2829 [00:00<00:00, 128551.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully assigned transcripts to bins for patch_10_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the transcripra_df chunks and generate gene-to-bin assignments \n",
    "patch_size = 1000\n",
    "bin_size = 2\n",
    "transcripts_df_chunks = os.listdir(transcripts_df_chunks_dir)\n",
    "for chunk_fname in transcripts_df_chunks:\n",
    "    output_loc = os.path.join(output_dir, chunk_fname)\n",
    "    # if os.path.exists(output_loc):\n",
    "    #     continue\n",
    "    if chunk_fname in [\".ipynb_checkpoints\"]:\n",
    "        continue\n",
    "    transcripts_df_chunk = pd.read_csv(os.path.join(transcripts_df_chunks_dir, chunk_fname))\n",
    "    bin_df_chunk = generate_synthetic_VisiumHD_data(transcripts_df_chunk, bin_size, whole_cell=True, QScore20=True)\n",
    "    bin_df_chunk['column'] = bin_df_chunk['column']*2-1\n",
    "    bin_df_chunk['row'] = bin_df_chunk['row']*2-1\n",
    "    bin_df_chunk['geometry'] = generate_bin_polys(bin_df_chunk, 'column', 'row', 2)\n",
    "    bin_gdf_chunk = gpd.GeoDataFrame( bin_df_chunk, geometry = bin_df_chunk['geometry'])\n",
    "    bin_df_chunk.to_csv(output_loc)\n",
    "    print(f\"Successfully assigned transcripts to bins for {chunk_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e310d-2a9d-41b5-9450-23ab3e57e7f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate cell_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d33fd-45be-4dde-b4b9-acc3de13f9e0",
   "metadata": {},
   "source": [
    "This session generate the cell_df patches required to run the enact pipeline. The main purpose is to create Shapely polygons that represent the cell outline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ceff4d",
   "metadata": {},
   "source": [
    "为什么要转换为Shapely多边形对象\n",
    "\n",
    "\n",
    "几何操作: Shapely多边形对象提供了丰富的几何操作功能，如计算面积、周长、交集、并集、包含关系等。这些功能在空间转录组数据分析中非常有用，例如计算多边形之间的重叠区域，或者判断一个细胞是否位于某个多边形内。\n",
    "空间分析: 使用多边形对象可以更容易地进行空间分析和可视化。在单细胞空间转录组数据分析中，理解细胞在组织切片中的空间分布是非常重要的。\n",
    "\n",
    "与其他库的兼容性: Shapely多边形对象与其他空间分析相关的库（如GeoPandas）兼容良好。通过将多边形对象添加到GeoDataFrame中，可以方便地进行进一步的空间分析和处理。\n",
    "\n",
    "总结来说，这段代码通过将坐标数组转换为Shapely多边形对象，并将其添加到DataFrame中，为后续的空间分析和可视化提供了几何基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a40a3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved patch_0_0.csv\n",
      "Saved patch_0_1.csv\n",
      "Saved patch_0_2.csv\n",
      "Saved patch_0_3.csv\n",
      "Saved patch_1_0.csv\n",
      "Saved patch_1_1.csv\n",
      "Saved patch_1_2.csv\n",
      "Saved patch_1_3.csv\n",
      "Saved patch_2_0.csv\n",
      "Saved patch_2_1.csv\n",
      "Saved patch_2_2.csv\n",
      "Saved patch_2_3.csv\n",
      "Saved patch_3_0.csv\n",
      "Saved patch_3_1.csv\n",
      "Saved patch_3_2.csv\n",
      "Saved patch_3_3.csv\n",
      "Saved patch_4_0.csv\n",
      "Saved patch_4_1.csv\n",
      "Saved patch_4_2.csv\n",
      "Saved patch_4_3.csv\n",
      "Saved patch_4_4.csv\n",
      "Saved patch_5_0.csv\n",
      "Saved patch_5_1.csv\n",
      "Saved patch_5_2.csv\n",
      "Saved patch_5_3.csv\n",
      "Saved patch_5_4.csv\n",
      "Saved patch_6_0.csv\n",
      "Saved patch_6_1.csv\n",
      "Saved patch_6_2.csv\n",
      "Saved patch_6_3.csv\n",
      "Saved patch_6_4.csv\n",
      "Saved patch_7_0.csv\n",
      "Saved patch_7_1.csv\n",
      "Saved patch_7_2.csv\n",
      "Saved patch_7_3.csv\n",
      "Saved patch_7_4.csv\n",
      "Saved patch_8_0.csv\n",
      "Saved patch_8_1.csv\n",
      "Saved patch_8_2.csv\n",
      "Saved patch_8_3.csv\n",
      "Saved patch_8_4.csv\n",
      "Saved patch_9_0.csv\n",
      "Saved patch_9_1.csv\n",
      "Saved patch_9_2.csv\n",
      "Saved patch_9_3.csv\n",
      "Saved patch_9_4.csv\n",
      "Saved patch_10_1.csv\n",
      "Saved patch_10_2.csv\n",
      "Saved patch_10_3.csv\n",
      "Saved patch_10_4.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Assuming cells_df is already loaded and contains 'polygons' column\n",
    "# Create polygons if not already done\n",
    "def create_polygons(coords_array):\n",
    "    polygons = []\n",
    "    for row in coords_array:\n",
    "        reshaped_coords = row.reshape(-1, 2)\n",
    "        polygon = Polygon(reshaped_coords)\n",
    "        polygons.append(polygon)\n",
    "    return polygons\n",
    "\n",
    "polygons = create_polygons(file)\n",
    "cells_df['polygons'] = polygons\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "cell_gdf = gpd.GeoDataFrame(cells_df, geometry=cells_df['polygons'])\n",
    "cell_gdf.rename(columns={'x_centroid': 'cell_x', 'y_centroid': 'cell_y'}, inplace=True)\n",
    "\n",
    "# Define patch size (same as transcripts)\n",
    "patch_size = 1000\n",
    "\n",
    "# Assign patch indices to cells based on their centroids\n",
    "cell_gdf['x_patch'] = (cell_gdf['cell_x'] // patch_size).astype(int)\n",
    "cell_gdf['y_patch'] = (cell_gdf['cell_y'] // patch_size).astype(int)\n",
    "cell_gdf['patch_id'] = cell_gdf['x_patch'].astype(str) + \"_\" + cell_gdf['y_patch'].astype(str)\n",
    "\n",
    "# Group by patch_id and save each group as a separate file\n",
    "grouped = cell_gdf.groupby(['x_patch', 'y_patch'])\n",
    "for (x_patch, y_patch), group in grouped:\n",
    "    filename = f\"patch_{x_patch}_{y_patch}.csv\"\n",
    "    output_loc = os.path.join(cells_df_chunks_dir, filename)\n",
    "    \n",
    "    # Save only the required columns\n",
    "    group[['cell_id', 'cell_x', 'cell_y', 'geometry']].to_csv(output_loc, index=False)\n",
    "    print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8aa8e-0a17-48ae-86ed-81a04ec203dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670974eb-8dae-4d67-b735-1cd53858d560",
   "metadata": {},
   "source": [
    "The following cell will generate and save the ground truth of the synthetic VisiumHD data for the use of bin-to-cell assignment methods evaluation. Ground truth dataframe consists of rows representing the transcript counts of each cell. Each column represents a gene feature (gene feature name is also the column name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224ea02-5701-450c-9efb-c38de7492764",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate Cell-gene matrix for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c214825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ground_truth_table(transcripts_df, cells_df, whole_cell=True, QScore20=True, include_unassigned_transcript=False):\n",
    "    filtered_df = transcripts_df\n",
    "    \n",
    "    # only count transcripts in the nucleus\n",
    "    if not whole_cell:\n",
    "        filtered_df = transcripts_df[transcripts_df['overlaps_nucleus'] == 1]\n",
    "    \n",
    "    # only count transcripts with QScore >= 20\n",
    "    if QScore20:\n",
    "        filtered_df = filtered_df[filtered_df['qv'] >= 20]\n",
    "    \n",
    "    # only count transcripts that are assigned to specific cells\n",
    "    if not include_unassigned_transcript:\n",
    "        filtered_df = filtered_df[filtered_df['cell_id'] != 'UNASSIGNED']\n",
    "    \n",
    "    pivot_df = filtered_df.pivot_table(index='cell_id', columns='feature_name', aggfunc='size', fill_value=0)\n",
    "    \n",
    "    merged_df = pivot_df.merge(cells_df[['cell_id']], left_index=True, right_on='cell_id', how='right')\n",
    "    columns = ['cell_id'] + [col for col in merged_df.columns if col not in ['cell_id', 'x_centroid', 'y_centroid','polygons']]\n",
    "    merged_df = merged_df[columns]\n",
    "    merged_df.set_index('cell_id', inplace=True)\n",
    "    #merged_df['total_gene_counts'] = merged_df.iloc[:, 3:].sum(axis=1)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06646b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated groundthuth for patch_0_0.csv\n",
      "Successfully generated groundthuth for patch_0_1.csv\n",
      "Successfully generated groundthuth for patch_0_2.csv\n",
      "Successfully generated groundthuth for patch_0_3.csv\n",
      "Successfully generated groundthuth for patch_1_0.csv\n",
      "Successfully generated groundthuth for patch_1_1.csv\n",
      "Successfully generated groundthuth for patch_1_2.csv\n",
      "Successfully generated groundthuth for patch_1_3.csv\n",
      "Successfully generated groundthuth for patch_2_0.csv\n",
      "Successfully generated groundthuth for patch_2_1.csv\n",
      "Successfully generated groundthuth for patch_2_2.csv\n",
      "Successfully generated groundthuth for patch_2_3.csv\n",
      "Successfully generated groundthuth for patch_3_0.csv\n",
      "Successfully generated groundthuth for patch_3_1.csv\n",
      "Successfully generated groundthuth for patch_3_2.csv\n",
      "Successfully generated groundthuth for patch_3_3.csv\n",
      "Successfully generated groundthuth for patch_4_0.csv\n",
      "Successfully generated groundthuth for patch_4_1.csv\n",
      "Successfully generated groundthuth for patch_4_2.csv\n",
      "Successfully generated groundthuth for patch_4_3.csv\n",
      "Successfully generated groundthuth for patch_4_4.csv\n",
      "Successfully generated groundthuth for patch_5_0.csv\n",
      "Successfully generated groundthuth for patch_5_1.csv\n",
      "Successfully generated groundthuth for patch_5_2.csv\n",
      "Successfully generated groundthuth for patch_5_3.csv\n",
      "Successfully generated groundthuth for patch_5_4.csv\n",
      "Successfully generated groundthuth for patch_6_0.csv\n",
      "Successfully generated groundthuth for patch_6_1.csv\n",
      "Successfully generated groundthuth for patch_6_2.csv\n",
      "Successfully generated groundthuth for patch_6_3.csv\n",
      "Successfully generated groundthuth for patch_6_4.csv\n",
      "Successfully generated groundthuth for patch_7_0.csv\n",
      "Successfully generated groundthuth for patch_7_1.csv\n",
      "Successfully generated groundthuth for patch_7_2.csv\n",
      "Successfully generated groundthuth for patch_7_3.csv\n",
      "Successfully generated groundthuth for patch_7_4.csv\n",
      "Successfully generated groundthuth for patch_8_0.csv\n",
      "Successfully generated groundthuth for patch_8_1.csv\n",
      "Successfully generated groundthuth for patch_8_2.csv\n",
      "Successfully generated groundthuth for patch_8_3.csv\n",
      "Successfully generated groundthuth for patch_8_4.csv\n",
      "Successfully generated groundthuth for patch_9_0.csv\n",
      "Successfully generated groundthuth for patch_9_1.csv\n",
      "Successfully generated groundthuth for patch_9_2.csv\n",
      "Successfully generated groundthuth for patch_9_3.csv\n",
      "Successfully generated groundthuth for patch_9_4.csv\n",
      "Successfully generated groundthuth for patch_10_1.csv\n",
      "Successfully generated groundthuth for patch_10_2.csv\n",
      "Successfully generated groundthuth for patch_10_3.csv\n",
      "Successfully generated groundthuth for patch_10_4.csv\n"
     ]
    }
   ],
   "source": [
    "bin_size = 2\n",
    "cell_df_chunks = os.listdir(cells_df_chunks_dir)\n",
    "for chunk_fname in cell_df_chunks:\n",
    "    output_loc = os.path.join(ground_truth_dir,chunk_fname)\n",
    "    if os.path.exists(output_loc):\n",
    "        continue\n",
    "    if chunk_fname in [\".ipynb_checkpoints\"]:\n",
    "        continue\n",
    "    cell_df_chunk = pd.read_csv(os.path.join(cells_df_chunks_dir, chunk_fname))\n",
    "    groundtruth_chunk = generate_ground_truth_table(transcripts_df, cell_df_chunk, whole_cell=False, QScore20=False, include_unassigned_transcript=False)\n",
    "    groundtruth_chunk.to_csv(output_loc)\n",
    "    print(f\"Successfully generated groundthuth for {chunk_fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4b13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
